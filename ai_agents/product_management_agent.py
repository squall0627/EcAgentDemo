import os
import json
from typing import TypedDict, Annotated, List, Dict, Any, Optional

from dotenv import load_dotenv
from langchain_ollama import ChatOllama
from langgraph.constants import START
from langfuse import Langfuse
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage, AIMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from ai_agents.tools.product_tools import (
    UpdateStockTool,
    UpdatePriceTool,
    UpdateDescriptionTool,
    UpdateCategoryTool,
    BulkUpdateStockTool,
    BulkUpdatePriceTool,
    ValidateCanPublishProductTool,
    GenerateHtmlTool,
    PublishProductsTool,
    UnpublishProductsTool, 
    search_products_tool
)
from config.llm_config_loader import llm_config

load_dotenv()

# Langfuse V3 ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
LANGFUSE_AVAILABLE = False

try:
    from langfuse.langchain import CallbackHandler
    from langfuse import observe
    LANGFUSE_AVAILABLE = True
    print("âœ… Langfuse V3 CallbackHandleræ­£å¸¸ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¾ã—ãŸ")
except ImportError as e:
    print(f"âŒ Langfuse V3 CallbackHandlerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")
    LANGFUSE_AVAILABLE = False

# LangGraphçŠ¶æ…‹å®šç¾© - ã‚ˆã‚ŠæŸ”è»ŸãªçŠ¶æ…‹ç®¡ç†
class AgentState(TypedDict):
    messages: Annotated[List[HumanMessage | AIMessage | SystemMessage], add_messages]
    user_input: str
    html_content: Optional[str]
    error_message: Optional[str]
    next_actions: Optional[str]
    session_id: Optional[str]
    user_id: Optional[str]

class ProductManagementAgent:
    def __init__(self, api_key: str, llm_type: str = None, use_langfuse: bool = True):
        """æŸ”è»Ÿãªå•†å“ç®¡ç†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ - è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®å‹•çš„LLMé¸æŠ"""
        self.api_key = api_key
        self.llm_type = llm_type or llm_config.get_default_model()
        self.use_langfuse = use_langfuse and LANGFUSE_AVAILABLE
        
        # Langfuse V3 åˆæœŸåŒ–
        self.langfuse_handler = None
        if self.use_langfuse:
            try:
                public_key = os.getenv("LANGFUSE_PUBLIC_KEY")
                secret_key = os.getenv("LANGFUSE_SECRET_KEY")
                host = os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com")
                if public_key and secret_key:
                    Langfuse(
                        public_key=public_key,
                        secret_key=secret_key,
                        host=host,
                    )
                    self.langfuse_handler = CallbackHandler()
                    print("âœ… Langfuse V3ãŒæ­£å¸¸ã«åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")
                else:
                    print("âš ï¸  Langfuseèªè¨¼æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    self.use_langfuse = False
            except Exception as e:
                print(f"âŒ LangfuseåˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                self.use_langfuse = False

        # å‹•çš„LLMåˆæœŸåŒ–
        self.llm = self._initialize_llm()
        
        # ãƒ„ãƒ¼ãƒ«åˆæœŸåŒ–
        self.tools = [
            search_products_tool,
            UpdateStockTool(),
            UpdatePriceTool(),
            UpdateDescriptionTool(),
            UpdateCategoryTool(),
            BulkUpdateStockTool(),
            BulkUpdatePriceTool(),
            ValidateCanPublishProductTool(),
            GenerateHtmlTool(),
            PublishProductsTool(),
            UnpublishProductsTool()
        ]
        
        # ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
        self.tool_map = {tool.name: tool for tool in self.tools}
        
        # ãƒ„ãƒ¼ãƒ«ä»˜ãLLM
        self.llm_with_tools = self.llm.bind_tools(self.tools)
        self.tool_node = ToolNode(self.tools)
        
        # æŸ”è»Ÿãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰
        self.graph = self._build_flexible_graph()

    def _initialize_llm(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«åŸºã¥ã„ã¦LLMã‚’åˆæœŸåŒ–"""
        # ãƒ¢ãƒ‡ãƒ«åˆ©ç”¨å¯èƒ½æ€§ã‚’æ¤œè¨¼
        is_available, message = llm_config.validate_model_availability(self.llm_type)
        if not is_available:
            print(f"âš ï¸ {message}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            self.llm_type = llm_config.get_default_model()
            print(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ« {self.llm_type} ã‚’ä½¿ç”¨ã—ã¾ã™")
        
        model_config = llm_config.get_model_config(self.llm_type)
        if not model_config:
            raise ValueError(f"ãƒ¢ãƒ‡ãƒ«è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.llm_type}")
        
        provider = model_config["provider"]
        model_name = model_config["model"]
        temperature = model_config.get("temperature", 0.7)
        
        try:
            if provider == "ollama":
                print(f"ğŸ¦™ Ollama LLM ({model_name}) ã‚’åˆæœŸåŒ–ä¸­...")
                return ChatOllama(
                    model=model_name,
                    base_url=model_config.get("base_url", "http://localhost:11434"),
                    temperature=temperature,
                )
            elif provider == "openai":
                print(f"ğŸ¤– OpenAI LLM ({model_name}) ã‚’åˆæœŸåŒ–ä¸­...")
                return ChatOpenAI(
                    openai_api_key=self.api_key,
                    model=model_name,
                    temperature=temperature
                )
            elif provider == "anthropic":
                print(f"ğŸ§  Anthropic LLM ({model_name}) ã‚’åˆæœŸåŒ–ä¸­...")
                try:
                    from langchain_anthropic import ChatAnthropic
                    return ChatAnthropic(
                        anthropic_api_key=os.getenv("ANTHROPIC_API_KEY"),
                        model=model_name,
                        temperature=temperature
                    )
                except ImportError:
                    print("âŒ langchain_anthropicãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                    return self._fallback_to_default()
            else:
                print(f"âŒ æœªå¯¾å¿œã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}")
                return self._fallback_to_default()
                
        except Exception as e:
            print(f"âŒ LLMåˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return self._fallback_to_default()

    def _fallback_to_default(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        default_model = llm_config.get_default_model()
        default_config = llm_config.get_model_config(default_model)
        
        print(f"ğŸ”„ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ« {default_model} ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
        
        if default_config["provider"] == "ollama":
            return ChatOllama(
                model=default_config["model"],
                base_url=default_config.get("base_url", "http://localhost:11434"),
                temperature=default_config.get("temperature", 0.7),
            )
        else:
            # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦OpenAI
            return ChatOpenAI(
                openai_api_key=self.api_key,
                model="gpt-4o-mini",
                temperature=0.1
            )

    def switch_llm(self, new_llm_type: str):
        """å®Ÿè¡Œæ™‚ã«LLMã‚¿ã‚¤ãƒ—ã‚’åˆ‡ã‚Šæ›¿ãˆ"""
        if new_llm_type != self.llm_type:
            old_type = self.llm_type
            self.llm_type = new_llm_type
            self.llm = self._initialize_llm()
            self.llm_with_tools = self.llm.bind_tools(self.tools)
            
            new_config = llm_config.get_model_config(new_llm_type)
            if new_config:
                print(f"âœ… LLMã‚’{old_type}ã‹ã‚‰{new_config['provider']}:{new_config['model']}ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")
            else:
                print(f"âœ… LLMã‚’{new_llm_type}ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")

    def get_available_llms(self):
        """åˆ©ç”¨å¯èƒ½ãªLLMã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return [model["value"] for model in llm_config.get_all_models()]
    
    def get_llm_info(self, llm_type: str = None):
        """LLMæƒ…å ±ã‚’å–å¾—"""
        target_type = llm_type or self.llm_type
        model_config = llm_config.get_model_config(target_type)
        
        if model_config:
            return {
                "type": target_type,
                "provider": model_config.get("provider", "unknown"),
                "model": model_config.get("model", "unknown"),
                "temperature": model_config.get("temperature", 0.7),
                "label": model_config.get("label", target_type),
                "description": model_config.get("description", "")
            }
        else:
            return {
                "type": target_type,
                "provider": "unknown",
                "model": "unknown",
                "temperature": 0.7,
                "label": target_type,
                "description": "è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            }

    def assistant(self, state: AgentState):
        # System message
        sys_msg = SystemMessage(
            content="""
ã‚ãªãŸã¯ECãƒãƒƒã‚¯ã‚ªãƒ•ã‚£ã‚¹å•†å“ç®¡ç†ã®å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ç®¡ç†è€…ã®è‡ªç„¶è¨€èªã‚³ãƒãƒ³ãƒ‰ã‚’ç†è§£ã—ã€ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ï¼š

## ä¸»è¦æ©Ÿèƒ½ï¼š
1. **å•†å“æ¤œç´¢**: è‡ªç„¶è¨€èªã§å•†å“ã‚’æ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
2. **å•†å“æ£šä¸Šã’ãƒ»æ£šä¸‹ã’ç®¡ç†**: å•†å“ã®æ£šä¸Šã’ãƒ»æ£šä¸‹ã’çŠ¶æ…‹ã‚’ç®¡ç†
3. **å•†å“åœ¨åº«ç®¡ç†**: å•†å“ã®åœ¨åº«çŠ¶æ…‹ã‚’ç®¡ç†
4. **å•†å“ä¾¡æ ¼ç®¡ç†**: å•†å“ã®ä¾¡æ ¼è¨­å®šãƒ»æ›´æ–°ã‚’ç®¡ç†ï¼ˆå€‹åˆ¥ãƒ»ä¸€æ‹¬å¯¾å¿œï¼‰
5. **å•†å“èª¬æ˜ç®¡ç†**: å•†å“ã®èª¬æ˜æ–‡ã‚’ç®¡ç†ãƒ»æ›´æ–°
6. **å‹•çš„HTMLç”Ÿæˆ**: æ“ä½œã«å¿œã˜ãŸç®¡ç†ç”»é¢ã‚’è‡ªå‹•ç”Ÿæˆ
7. **ã‚¨ãƒ©ãƒ¼å‡¦ç†ã¨èª˜å°**: å•é¡Œè§£æ±ºã¾ã§æ®µéšçš„ã«ã‚µãƒãƒ¼ãƒˆ

## å•†å“æ£šä¸Šã’ã®å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯ï¼š
å•†å“ã‚’æ£šä¸Šã’ã™ã‚‹å‰ã«ã€å¿…ãšä»¥ä¸‹ã®æ¡ä»¶ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š
- âœ… å•†å“ã‚«ãƒ†ã‚´ãƒªãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ï¼ˆnull ã¾ãŸã¯ç©ºæ–‡å­—åˆ—ã§ã¯ãªã„ï¼‰
- âœ… å•†å“åœ¨åº«ãŒ0ã‚ˆã‚Šå¤§ãã„
- âœ… å•†å“ä¾¡æ ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ï¼ˆ0ã‚ˆã‚Šå¤§ãã„ï¼‰

## HTMLç”Ÿæˆãƒ«ãƒ¼ãƒ«ï¼š
- å•†å“ãƒªã‚¹ãƒˆè¡¨ç¤ºï¼šæ¤œç´¢çµæœã‚’è¡¨å½¢å¼ã§è¡¨ç¤ºã€å„å•†å“ã«æ“ä½œãƒœã‚¿ãƒ³ä»˜ã
- ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨­å®šç”»é¢ï¼šãƒ•ã‚©ãƒ¼ãƒ å½¢å¼ã§ã‚«ãƒ†ã‚´ãƒªãƒ¼é¸æŠãƒ»å…¥åŠ›
- åœ¨åº«ç®¡ç†ç”»é¢ï¼šæ•°å€¤å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã§åœ¨åº«æ•°é‡è¨­å®š
- ä¾¡æ ¼ç®¡ç†ç”»é¢ï¼šæ•°å€¤å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã§ä¾¡æ ¼è¨­å®šï¼ˆé€šè²¨è¡¨ç¤ºå¯¾å¿œï¼‰
- å•†å“èª¬æ˜ç®¡ç†ç”»é¢ï¼šãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã§èª¬æ˜æ–‡ç·¨é›†
- ã‚¨ãƒ©ãƒ¼ç”»é¢ï¼šå•é¡Œç‚¹ã‚’æ˜ç¤ºã—ã€è§£æ±ºæ–¹æ³•ã‚’æç¤º

## ä¾¡æ ¼ã«é–¢ã™ã‚‹æ³¨æ„äº‹é …ï¼š
- ä¾¡æ ¼ã¯å¿…ãš0ä»¥ä¸Šã®å€¤ã‚’è¨­å®šã—ã¦ãã ã•ã„
- ä¾¡æ ¼è¡¨ç¤ºã¯é€šè²¨å½¢å¼ï¼ˆÂ¥1,234.56ï¼‰ã§è¡¨ç¤º
- ä¸€æ‹¬ä¾¡æ ¼æ›´æ–°ã§ã¯ã€ç„¡åŠ¹ãªä¾¡æ ¼å€¤ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦å‡¦ç†ç¶šè¡Œ

## é‡è¦ãªå‹•ä½œåŸå‰‡ï¼š
1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå•é¡Œè§£æ±ºã¾ã§æ®µéšçš„ã«ã‚µãƒãƒ¼ãƒˆ
2. æ¯å›å¿œç­”ã®æœ€å¾Œã€***å¿…ãš***é©åˆ‡ãªæ“ä½œç”»é¢(HTML)ã‚’è‡ªå‹•ç”Ÿæˆ

## å¿œç­”å½¢å¼ï¼š
- JSONå½¢å¼ã§æ§‹é€ åŒ–ã•ã‚ŒãŸå¿œç­”
- HTMLç”ŸæˆãŒå¿…è¦ãªå ´åˆã¯ "html_content" ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å«ã‚ã€ç›´æ¥ã«ç”»é¢ã«ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã—ã¦ãã ã•ã„
- ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ "error" ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«æ—¥æœ¬èªã§è¨˜è¼‰
- æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆã¯ "next_actions" ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å«ã‚ã‚‹

å¸¸ã«è¦ªã—ã¿ã‚„ã™ãæ˜ç¢ºãªæ—¥æœ¬èªã§å¿œç­”ã—ã€ç®¡ç†è€…ã®æ¥­å‹™åŠ¹ç‡å‘ä¸Šã‚’æœ€å„ªå…ˆã«è€ƒãˆã¦ãã ã•ã„ã€‚
""")

        state["messages"].append(self.llm_with_tools.invoke([sys_msg] + state["messages"]))
        return state

    def _build_flexible_graph(self) -> StateGraph:
        """æŸ”è»Ÿãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰"""
        # Define the state graph
        builder = StateGraph(AgentState)

        # Define nodes: these do the work
        builder.add_node("assistant", self.assistant)
        builder.add_node("tools", self.tool_node)

        # Define edges: these determine how the control flow moves
        builder.add_edge(START, "assistant")
        builder.add_conditional_edges(
            "assistant",
            # If the latest message requires a tool, route to tools
            # Otherwise, provide a direct response
            tools_condition,
        )
        builder.add_edge("tools", "assistant")
        
        return builder.compile()
    
    def _get_langfuse_config(self, step_name: str = None, session_id: str = None, user_id: str = None) -> Dict:
        """Langfuseè¨­å®šã‚’å–å¾—"""
        if self.use_langfuse and self.langfuse_handler:
            return {"callbacks": [self.langfuse_handler],
                    "metadata": {
                        "langfuse.user_id": user_id,
                        "langfuse.session_id": session_id
                    }}
        return {}

    @observe(name="product_management_workflow")
    def process_command(self, command: str, llm_type: str = None, session_id: str = None, user_id: str = None) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒãƒ³ãƒ‰ã‚’å‡¦ç† - è¨­å®šãƒ™ãƒ¼ã‚¹ã®å‹•çš„LLMåˆ‡ã‚Šæ›¿ãˆå¯¾å¿œ"""
        try:
            # LLMã‚¿ã‚¤ãƒ—ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯åˆ‡ã‚Šæ›¿ãˆ
            if llm_type and llm_type != self.llm_type:
                self.switch_llm(llm_type)
            
            # åˆæœŸçŠ¶æ…‹
            initial_state = AgentState(
                messages=[HumanMessage(content=command)],
                user_input=command,
                html_content=None,
                error_message=None,
                next_actions=None,
                session_id=session_id,
                user_id=user_id,
            )
            
            # æŸ”è»Ÿãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
            config = self._get_langfuse_config("product_management_workflow", session_id, user_id)
            final_state = self.graph.invoke(initial_state, config=config)
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ§‹ç¯‰
            response_data = {
                "message": final_state["messages"][-1].content if final_state["messages"] else "å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ",
                "html_content": final_state.get("html_content"),
                "next_actions": final_state.get("next_actions"),
                "llm_type_used": self.llm_type,
                "llm_info": self.get_llm_info()
            }
            
            if final_state.get("error_message"):
                response_data["error"] = final_state["error_message"]
            
            return json.dumps(response_data, ensure_ascii=False, indent=2)
            
        except Exception as e:
            error_msg = f"ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
            return json.dumps({
                "message": error_msg,
                "error": str(e),
                "llm_type_used": self.llm_type,
                "llm_info": self.get_llm_info()
            }, ensure_ascii=False)

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
EXAMPLE_COMMANDS = [
    # ç›´æ¥å®Ÿè¡Œã‚¿ã‚¤ãƒ—
    "JAN123456789ã®åœ¨åº«ã‚’50ã«å¤‰æ›´",
    "å•†å“987654321ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’é£²æ–™ã«å¤‰æ›´",
    "JAN123456789ã®ä¾¡æ ¼ã‚’1500å††ã«è¨­å®š",
    "å•†å“987654321ã®å•†å“èª¬æ˜ã‚’æ›´æ–°",

    # æ¤œç´¢å¾Œå®Ÿè¡Œã‚¿ã‚¤ãƒ—
    "ã™ã¹ã¦ã®ã‚³ãƒ¼ãƒ’ãƒ¼å•†å“ã®åœ¨åº«ã‚’100ã«å¤‰æ›´",
    "åœ¨åº«ä¸è¶³ã®å•†å“ã‚’ã™ã¹ã¦æ£šä¸‹ã’",
    "é£²æ–™ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®å•†å“ã‚’ã™ã¹ã¦æ£šä¸Šã’",
    "ä¾¡æ ¼ãŒ1000å††ä»¥ä¸‹ã®å•†å“ã‚’æ¤œç´¢",
    "èª¬æ˜æ–‡ã«ã€Œé™å®šã€ã‚’å«ã‚€å•†å“ã‚’æ¤œç´¢",

    # æ¤œè¨¼å¾Œå®Ÿè¡Œã‚¿ã‚¤ãƒ—
    "å•†å“ABC123ã‚’æ£šä¸Šã’",
    "JAN555666777ã‚’è²©å£²é–‹å§‹",

    # ãƒ•ã‚©ãƒ¼ãƒ ãŒå¿…è¦ãªã‚¿ã‚¤ãƒ—
    "å•†å“åœ¨åº«ã‚’ä¿®æ­£",
    "å•†å“æƒ…å ±ã‚’æ›´æ–°",
    "å•†å“ç®¡ç†",
    "å•†å“ä¾¡æ ¼ã‚’è¨­å®š",
    "å•†å“èª¬æ˜ã‚’ç·¨é›†"
]

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    api_key = os.getenv("OPENAI_API_KEY")
    
    # è¨­å®šãƒ™ãƒ¼ã‚¹ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–
    agent = ProductManagementAgent(api_key)
    
    # åˆ©ç”¨å¯èƒ½ãªLLMã‚’è¡¨ç¤º
    print("åˆ©ç”¨å¯èƒ½ãªLLM:", agent.get_available_llms())
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    result = agent.process_command("JAN code 1000000000001ã®å•†å“ã‚’æ¤œç´¢ã—ã€å•†å“è©³ç´°ä¸€è¦§ç”»é¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")
    print(result)