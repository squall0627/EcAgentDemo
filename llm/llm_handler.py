import os
from typing import Optional
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from config.llm_config_loader import llm_config


class LLMHandler:
    """
    æ±ç”¨LLMãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã€å„ç¨®LLMã®åˆæœŸåŒ–ã€åˆ‡ã‚Šæ›¿ãˆã€ç®¡ç†ã‚’æ‹…å½“
    ç•°ãªã‚‹Agentã§å†åˆ©ç”¨å¯èƒ½
    """
    
    def __init__(self, api_key: str, llm_type: Optional[str] = None):
        """
        LLMãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’åˆæœŸåŒ–
        
        Args:
            api_key: APIã‚­ãƒ¼ï¼ˆä¸»ã«OpenAIç”¨ï¼‰
            llm_type: LLMã‚¿ã‚¤ãƒ—ã€Noneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        """
        self.api_key = api_key
        self.llm_type = llm_type or llm_config.get_default_model()
        self.llm = self._initialize_llm()
    
    def _initialize_llm(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«åŸºã¥ã„ã¦LLMã‚’åˆæœŸåŒ–"""
        # ãƒ¢ãƒ‡ãƒ«åˆ©ç”¨å¯èƒ½æ€§ã‚’æ¤œè¨¼
        is_available, message = llm_config.validate_model_availability(self.llm_type)
        if not is_available:
            print(f"âš ï¸ {message}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            self.llm_type = llm_config.get_default_model()
            print(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ« {self.llm_type} ã‚’ä½¿ç”¨ã—ã¾ã™")
        
        model_config = llm_config.get_model_config(self.llm_type)
        if not model_config:
            raise ValueError(f"ãƒ¢ãƒ‡ãƒ«è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.llm_type}")
        
        provider = model_config["provider"]
        model_name = model_config["model"]
        temperature = model_config.get("temperature", 0.7)
        
        try:
            if provider == "ollama":
                print(f"ğŸ¦™ Ollama LLM ({model_name}) ã‚’åˆæœŸåŒ–ä¸­...")
                return ChatOllama(
                    model=model_name,
                    base_url=model_config.get("base_url", "http://localhost:11434"),
                    temperature=temperature,
                )
            elif provider == "openai":
                print(f"ğŸ¤– OpenAI LLM ({model_name}) ã‚’åˆæœŸåŒ–ä¸­...")
                return ChatOpenAI(
                    openai_api_key=self.api_key,
                    model=model_name,
                    temperature=temperature
                )
            elif provider == "anthropic":
                print(f"ğŸ§  Anthropic LLM ({model_name}) ã‚’åˆæœŸåŒ–ä¸­...")
                try:
                    from langchain_anthropic import ChatAnthropic
                    return ChatAnthropic(
                        anthropic_api_key=os.getenv("ANTHROPIC_API_KEY"),
                        model=model_name,
                        temperature=temperature
                    )
                except ImportError:
                    print("âŒ langchain_anthropicãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                    return self._fallback_to_default()
            else:
                print(f"âŒ æœªå¯¾å¿œã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}")
                return self._fallback_to_default()
                
        except Exception as e:
            print(f"âŒ LLMåˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return self._fallback_to_default()

    def _fallback_to_default(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        default_model = llm_config.get_default_model()
        default_config = llm_config.get_model_config(default_model)
        
        print(f"ğŸ”„ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ« {default_model} ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
        
        if default_config["provider"] == "ollama":
            return ChatOllama(
                model=default_config["model"],
                base_url=default_config.get("base_url", "http://localhost:11434"),
                temperature=default_config.get("temperature", 0.7),
            )
        else:
            # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦OpenAI
            return ChatOpenAI(
                openai_api_key=self.api_key,
                model="gpt-4o-mini",
                temperature=0.1
            )

    def switch_llm(self, new_llm_type: str):
        """å®Ÿè¡Œæ™‚ã«LLMã‚¿ã‚¤ãƒ—ã‚’åˆ‡ã‚Šæ›¿ãˆ"""
        if new_llm_type != self.llm_type:
            old_type = self.llm_type
            self.llm_type = new_llm_type
            self.llm = self._initialize_llm()
            
            new_config = llm_config.get_model_config(new_llm_type)
            if new_config:
                print(f"âœ… LLMã‚’{old_type}ã‹ã‚‰{new_config['provider']}:{new_config['model']}ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")
            else:
                print(f"âœ… LLMã‚’{new_llm_type}ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")

    def get_available_llms(self):
        """åˆ©ç”¨å¯èƒ½ãªLLMã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return [model["value"] for model in llm_config.get_all_models()]
    
    def get_llm_info(self, llm_type: Optional[str] = None):
        """LLMæƒ…å ±ã‚’å–å¾—"""
        target_type = llm_type or self.llm_type
        model_config = llm_config.get_model_config(target_type)
        
        if model_config:
            return {
                "type": target_type,
                "provider": model_config.get("provider", "unknown"),
                "model": model_config.get("model", "unknown"),
                "temperature": model_config.get("temperature", 0.7),
                "label": model_config.get("label", target_type),
                "description": model_config.get("description", "")
            }
        else:
            return {
                "type": target_type,
                "provider": "unknown",
                "model": "unknown",
                "temperature": 0.7,
                "label": target_type,
                "description": "è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            }
    
    def get_llm(self):
        """ç¾åœ¨ä½¿ç”¨ä¸­ã®LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
        return self.llm
    
    def get_llm_with_tools(self, tools):
        """ãƒ„ãƒ¼ãƒ«ä»˜ãLLMã‚’å–å¾—"""
        return self.llm.bind_tools(tools)
    
    def get_current_llm_type(self):
        """ç¾åœ¨ã®LLMã‚¿ã‚¤ãƒ—ã‚’å–å¾—"""
        return self.llm_type